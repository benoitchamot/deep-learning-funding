{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding name back as a feature\n",
    "After discussion with some peers, it seems that adding the name back as a feature improves the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#  Import and read the charity_data.csv.\n",
    "import pandas as pd \n",
    "application_df = pd.read_csv(\"https://static.bc-edx.com/data/dla-1-2/m21/lms/starter/charity_data.csv\")\n",
    "\n",
    "# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.\n",
    "application_df = application_df.drop(columns=['EIN'])\n",
    "\n",
    "# Choose a cutoff value of 500 and create a list of application types to be replaced\n",
    "all_app_types = application_df['APPLICATION_TYPE'].value_counts().to_dict()\n",
    "application_types_to_replace = [i for i in all_app_types.keys() if all_app_types[i] < 500]\n",
    "\n",
    "# Replace in dataframe\n",
    "for app in application_types_to_replace:\n",
    "    application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,\"Other\")\n",
    "\n",
    "# Choose a cutoff value and create a list of classifications to be replaced\n",
    "# use the variable name `classifications_to_replace`\n",
    "all_class_types = application_df['CLASSIFICATION'].value_counts().to_dict()\n",
    "classifications_to_replace = [i for i in all_class_types.keys() if all_class_types[i] < 1000]\n",
    "\n",
    "# Replace in dataframe\n",
    "for cls in classifications_to_replace:\n",
    "    application_df['CLASSIFICATION'] = application_df['CLASSIFICATION'].replace(cls,\"Other\")\n",
    "\n",
    "# Get all categorical data columns into a list\n",
    "dummies_cols = []\n",
    "\n",
    "cols = application_df.columns\n",
    "typs = application_df.dtypes.values\n",
    "\n",
    "for c in range(len(cols)):\n",
    "    if (typs[c] == 'object'):\n",
    "        dummies_cols.append(cols[c])\n",
    "\n",
    "# Convert categorical data to numeric with `pd.get_dummies`\n",
    "dummies_df = pd.get_dummies(application_df[dummies_cols])\n",
    "\n",
    "# Concatenate the DF with the dummy data with the original dataset\n",
    "concat_df = pd.concat([application_df, dummies_df], axis=1)\n",
    "\n",
    "# Drop the unused columns\n",
    "concat_df = concat_df.drop(columns=dummies_cols)\n",
    "\n",
    "# Split our preprocessed data into our features and target arrays\n",
    "y = concat_df['IS_SUCCESSFUL']\n",
    "X = concat_df.drop(columns='IS_SUCCESSFUL')\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)\n",
    "\n",
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19611"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model optimisation (with keras tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a method that creates a new Sequential model with hyperparameter options\n",
    "def create_model(hp):\n",
    "\n",
    "    input_dim = 19611\n",
    "\n",
    "\n",
    "    nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Allow kerastuner to decide which activation function to use in hidden layers\n",
    "    activation = hp.Choice('activation',['relu','selu','sigmoid','tanh'])\n",
    "    \n",
    "    # Allow kerastuner to decide number of neurons in first layer\n",
    "    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',\n",
    "        min_value=1,\n",
    "        max_value=20,\n",
    "        step=2), activation=activation, input_dim=input_dim))\n",
    "\n",
    "    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers\n",
    "    for i in range(hp.Int('num_layers', 1, 6)):\n",
    "        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
    "            min_value=1,\n",
    "            max_value=20,\n",
    "            step=2),\n",
    "            activation=activation))\n",
    "    \n",
    "    nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile the model\n",
    "    nn_model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "    \n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the kerastuner library\n",
    "import keras_tuner as kt\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    create_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_epochs=20,\n",
    "    hyperband_iterations=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60 Complete [00h 01m 07s]\n",
      "val_accuracy: 0.7969679236412048\n",
      "\n",
      "Best val_accuracy So Far: 0.8006997108459473\n",
      "Total elapsed time: 00h 34m 44s\n"
     ]
    }
   ],
   "source": [
    "# Run the kerastuner search for best hyperparameters\n",
    "tuner.search(X_train_scaled,y_train,epochs=20,validation_data=(X_test_scaled,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'sigmoid',\n",
       " 'first_units': 17,\n",
       " 'num_layers': 2,\n",
       " 'units_0': 19,\n",
       " 'tuner/epochs': 7,\n",
       " 'tuner/initial_epoch': 3,\n",
       " 'tuner/bracket': 2,\n",
       " 'tuner/round': 1,\n",
       " 'units_1': 1,\n",
       " 'tuner/trial_id': '0000',\n",
       " 'units_2': 17,\n",
       " 'units_3': 15,\n",
       " 'units_4': 5,\n",
       " 'units_5': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get best model hyperparameters\n",
    "best_hyper = tuner.get_best_hyperparameters(1)[0]\n",
    "best_hyper.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/268 - 1s - loss: 0.4876 - accuracy: 0.8007 - 590ms/epoch - 2ms/step\n",
      "Loss: 48.76%\n",
      "Accuracy: 80.07%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate best model against full test data\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "model_loss, model_accuracy = best_model.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "\n",
    "print(f\"Loss: {100*model_loss:.2f}%\")\n",
    "print(f\"Accuracy: {100*model_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
